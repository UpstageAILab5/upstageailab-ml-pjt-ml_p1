{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í›ˆë ¨ì‹œ íŒŒë¼ë¯¸í„°\n",
    "\n",
    "learning_rate: 2e-05\n",
    "train_batch_size: 32\n",
    "eval_batch_size: 32\n",
    "seed: 42\n",
    "gradient_accumulation_steps: 8\n",
    "total_train_batch_size: 256\n",
    "optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n",
    "lr_scheduler_type: linear\n",
    "lr_scheduler_warmup_ratio: 0.1\n",
    "num_epochs: 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Review_Text  Label\n",
      "0  í¬ì¥í•˜ë ¤ê³  ë“¤ì–´ì„  ìˆœê°„ë¶€í„° ë­”ê°€ ë§íˆ¬ê°€ ì§œì¦ë‚˜ìˆê³  ì‹¸ìš°ìëŠ” íƒœë„ì˜€ì§€ë§Œ ì¼ë‹¨ ì°¸ì•˜ëŠ”ë°...      0\n",
      "1  ì¬ì˜¤í”ˆ í•˜ì‹ ê±´ì§€ ê·¸ë™ì•ˆ ëª»ê°€ì„œ ì•„ì‰¬ì› ìŠµë‹ˆë‹¤ ê·¸ë¦¬ê³  ì•„ ì†”ì§íˆ ì—¬ê¸° ë³´ìŒˆì´ë‘ ì „ ê¹€ì¹˜...      1\n",
      "2  ë§›ê³¼ ë¶„ìœ„ê¸° ì¢‹ê³  ìŠ¤í…Œì´í¬ ê°€ì„±ë¹„ ì¢‹ìŠµë‹ˆë‹¤ ë‹¤ë§Œ ìƒê°ë³´ë‹¤ ë§ì´ ìµí˜€ì„œ ë‚˜ì˜µë‹ˆë‹¤ í‰ì†Œ...      1\n",
      "3                              ì¹œì ˆí•˜ê³  ë¶„ìœ„ê¸° ì¢‹ì€ë°ë§›ì´ì—†ì–´ìš” ë¹„ì‹¸ìš”      0\n",
      "4    ì‹œ ë¶„ì— ì™”ëŠ”ë° ì¬ë£Œ ì†Œì§„ìœ¼ë¡œ ë¬¸ ë‹«ì€ ê³³ì€ ì²¨ ë³´ë„¤ ì¥ì‚¬ë¥¼ í•˜ê³  ì‹¶ì„ ë•Œë§Œ í•˜ì‹œë‚˜ìš”      0\n",
      "5  ì§ì›ë¶„ë“¤ ë¶ˆì¹œì ˆí•˜ê³  ê°€ìœ„ì— ì´ë¬¼ì§ˆë¬»ì–´ì„œ êµí™˜ìš”ì²­í•˜ë‹ˆê¹Œ ì“°ìœ½ ë³´ì‹œë”ë‹ˆ ì™œì´ë ‡ê²Œ ì˜ˆë¯¼í•˜...      0\n",
      "6                                       ë„ˆë¬´ë§›ìˆì–´ìš”ì–‘ë„í‘¸ì§í•´ìš”      1\n",
      "7  ë°‘ì— ëª‡ ë¶„ë“¤ì´ ë§ì”€í•˜ì‹ ëŒ€ë¡œ í•œêº¼ë²ˆì— íŒ€ ì…ì¥ì‹œí‚¤ëŠ” ì‹œìŠ¤í…œ ì§„ì§œ ì§œì¦ë‚˜ìš” ë•ë¶„ì— ì‹œ...      0\n",
      "8                                      ì •ëˆì€ ì–¸ì œ ë¨¹ì–´ë„ ìµœê³       1\n",
      "9  ë™ë£Œì™€ í—˜ê»˜ ëˆê¹ŒìŠ¤ì™€ ê¹€ì¹˜ëˆê¹ŒìŠ¤ëšë°°ê¸°ë¥¼ ì£¼ë¬¸ë‘ê°œ ë©”ë‰´ ëª¨ë‘ ìì£¼ ë¨¹ë˜ ë©”ë‰´ì…ë‹ˆë‹¤ í•˜...      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('./data/KoBert_review_train_set_v2.03.csv')\n",
    "\n",
    "print(dataset.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ë¡œ 10ê°œë§Œ ì§„í–‰\n",
    "df = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraForSequenceClassification, ElectraTokenizer\n",
    "import torch\n",
    "\n",
    "# ê°ì„± ë¶„ì„ìš© ëª¨ë¸ ë¡œë“œ\n",
    "model_name = \"monologg/koelectra-base-v3-discriminator\"\n",
    "tokenizer = ElectraTokenizer.from_pretrained(model_name)\n",
    "model = ElectraForSequenceClassification.from_pretrained(model_name, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', '', text)\n",
    "    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ ë° í† í°í™”\n",
    "def tokenize_data(texts, tokenizer, max_length=128):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ ì ìš©\n",
    "dataset['clean_text'] = dataset['Review_Text'].apply(preprocess_text)\n",
    "\n",
    "# ì´ì œ train_test_split ì§„í–‰\n",
    "train_df, eval_df = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.texts = df['clean_text'].values  # clean_text ì‚¬ìš©\n",
    "        self.labels = df['Label'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):  # ì´ ë©”ì†Œë“œ ì¶”ê°€\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'][0],\n",
    "            'attention_mask': encoding['attention_mask'][0],\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ê°ì²´ ìƒì„±\n",
    "train_dataset = ReviewDataset(train_df, tokenizer)\n",
    "eval_dataset = ReviewDataset(eval_df, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/05 07:01:03 INFO mlflow.tracking.fluent: Experiment with name 'sentiment_analysis' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/data/ephemeral/home/mlruns/1', creation_time=1733382063071, experiment_id='1', last_update_time=1733382063071, lifecycle_stage='active', name='sentiment_analysis', tags={}>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLflow ì„¤ì •\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://10.196.197.32:30164\")  # MLflow ì„œë²„ URI ì„¤ì •\n",
    "mlflow.set_experiment(\"sentiment_analysis\")  # ì‹¤í—˜ ì´ë¦„ ì„¤ì •\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµ ì¸ì ì„¤ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_ratio=0.1,\n",
    "    seed=42,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í‰ê°€ ë©”íŠ¸ë¦­ ì •ì˜\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    accuracy = np.mean(labels == preds)\n",
    "    return {'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Trainer ì´ˆê¸°í™” ë° í•™ìŠµ ì‹œì‘\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/05 07:01:16 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial message can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "    - error|e|exception|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='810' max='810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [810/810 18:11, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.285979</td>\n",
       "      <td>0.910735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.212622</td>\n",
       "      <td>0.928242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.189446</td>\n",
       "      <td>0.933436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.199275</td>\n",
       "      <td>0.937476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.210076</td>\n",
       "      <td>0.937668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.209300</td>\n",
       "      <td>0.223456</td>\n",
       "      <td>0.936322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.209300</td>\n",
       "      <td>0.232980</td>\n",
       "      <td>0.936899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.209300</td>\n",
       "      <td>0.234677</td>\n",
       "      <td>0.936706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run ./results at: http://10.196.197.32:30164/#/experiments/1/runs/f7509293e2844adbb925916b1edfdf24\n",
      "ğŸ§ª View experiment at: http://10.196.197.32:30164/#/experiments/1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=810, training_loss=0.1558755898181303, metrics={'train_runtime': 1092.0318, 'train_samples_per_second': 190.397, 'train_steps_per_second': 0.742, 'total_flos': 1.363494111086592e+16, 'train_loss': 0.1558755898181303, 'epoch': 9.96923076923077})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ëª¨ë¸ í•™ìŠµ\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_model/tokenizer_config.json',\n",
       " './saved_model/special_tokens_map.json',\n",
       " './saved_model/vocab.txt',\n",
       " './saved_model/added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í•™ìŠµëœ ëª¨ë¸ ì €ì¥\n",
    "model.save_pretrained('./saved_model')\n",
    "tokenizer.save_pretrained('./saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
